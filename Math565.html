<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <script type="text/javascript"
	    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <meta name="generator" content="BKStyle" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="BKStyle.css" type="text/css" />
    <title>Optimization for Machine Learning (Math 565)</title>
  </head>

  <body>
    <table summary="Table for page layout." id="tlayout">
      <tr valign="top">
	<td id="layout-menu">
	  <div class="menu-item"><a href="Math565.html"><big>Math565 Home</big></a></div>
	  <div class="menu-item"><a href="FilesMath565/S26/LecNotes/index.html"><big>Lectures</big></a></div>
	  <div class="menu-item"><a href="FilesMath565/S26/Homeworks/index.html"><big>Homework</big></a></div>
	  <div class="menu-item"><a href="FilesMath565/S26/Handouts/index.html"><big>Handouts</big></a></div>
	  <div class="menu-item"><a href="FilesMath565/S26/Software/index.html"><big>Software</big></a></div>	  
	  <div class="menu-category"><br /></div>
	  <div class="menu-item"><a href="index.html" class="current"><big>Bala's Home</big></a></div>
	</td>

	<td id="layout-content">
	  <h1><b>Math 565: Optimization for Machine Learning</b></h1>

	    <h2>Course Description</h2>

	    <table border="0" width="75%" >
	      <tr>
		<td align=left> 
		  <p>
		    This course will offer a systematic treatment of
		    optimization methods with a focus on applications
		    in machine learning (ML).  There are key
		    differences between optimization as used in ML and
		    traditional optimization&mdash;for instance, the
		    performance of ML models are assessed based on how
		    well they generalize to <i>test data</i> (rather
		    than on the whole dataset).  For instance, the
		    widely used stochastic gradient-descent methods
		    could have lower accuracy than gradient-descent on
		    training data but often perform better on test
		    data!
		  </p>

		  <p>
		    We will cover a selection of relevant topics from
		    the book
		    titled <a href="https://link.springer.com/book/10.1007/978-3-030-40344-7">Linear
		    Algebra and Optimization for Machine Learning</a>
		    by Charu Aggarwal, including gradient and
		    stochastic gradient descent, Newton method in ML
		    problems such as regression and support vector
		    machines (SVM), Lagrangian relaxation and duality
		    for SVM, penalty-based methods, optimization in
		    computational graphs including neural networks
		    (NNs), and backpropagation in NNs.
		  </p>

		  <p>
		    Homework assignments will include proof-type
		    problems as well as ones needing use of software.
		    Students will also work on a computational
		    project.  No exams will be given.  This course
		    will (ideally) be a follow-up of Math564:
		    Nonlinear Optimization (offered in Fall 2025).
		    Students who took that course will be best
		    prepared to do well in Math 565.  Independent of
		    Math564, prerequisites for Math565 are familiarity
		    with analysis and linear algebra with proof-based
		    work at the undergraduate (400-) level, <b>or
		    obtain the permission of the instructor</b>.
		    Familiarity with computer programming languages or
		    packages such as Matlab or Python will also be
		    expected.
		  </p>

		</td>
	      </tr>
	    </table>

	    
	  <p>
	    <h2><a 
	    href="FilesMath565/S26/SyllSched/Opt4ML_Math565_S26_Syllabus.pdf">Syllabus</a></h2>
	  </p>

	  <h2>Announcements</h2>

	  <table>
	    <tr>
              <td>
		<b>Mon, Jan 12:</b> The class will meet in VECS 120
		(Vancouver) and Sloan 7 (Pullman).
              </td>
	    </tr>


	  </table>

	  <p>
	    <br>
          </p>
	  
	</td>
      </tr>
    </table>

  </body>
</html>	  
